# -*- coding: utf-8 -*-
"""COS429 Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sdf8JN9nwWmzRLILj8WHHiwTGQrPg-Ai

Loading the Cityscapes dataset
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
print(os.getcwd())

# Commented out IPython magic to ensure Python compatibility.
# can edit
# %cd gdrive/MyDrive/classes/COS429/Final Project
#%cd gtFine_trainvaltest/gtFine
# %ls

from google.colab import drive
drive.mount('/content/drive')

from torch.utils.data import Dataset
from PIL import Image
import numpy as np
import torch

class CityscapesDataset(Dataset):
  def __init__(self, tvt, image_transform=None, mask_transform=None):
    self.image_transform = image_transform
    self.mask_transform = mask_transform
    self.images = []
    self.masks = []
    self.tvt = tvt #0 for train, 1 for val, 2 for test
    if(tvt == 0):
      s = "train"
    elif(tvt == 1):
      s = "val"
    else:
      s = "test"
    img_root = os.path.join("leftImg8bit", s)
    mask_root = os.path.join("gtFine_trainvaltest/gtFine", s)

    for city in os.listdir(img_root):
      img_dir = os.path.join(img_root, city)
      mask_dir = os.path.join(mask_root, city)

      for file_name in os.listdir(img_dir):
        if(file_name.endswith("_leftImg8bit.png")):
          img_path = os.path.join(img_dir, file_name)
          mask_name = file_name.replace("_leftImg8bit.png", "_gtFine_labelIds.png")
          mask_path = os.path.join(mask_dir, mask_name)

          if(os.path.isfile(img_path) and os.path.isfile(mask_path)):
            self.images.append(img_path)
            self.masks.append(mask_path)

  def __len__(self):
    return len(self.images)

  def __getitem__(self, id):
    image = Image.open(self.images[id]).convert("RGB")
    mask = Image.open(self.masks[id])

    if(self.image_transform is not None):
      image = self.image_transform(image)

    if(self.mask_transform is not None):
      mask = self.mask_transform(mask)



    return image, mask

from torch.utils.data import DataLoader
from torchvision import transforms
import os # Import os module

# The previous cell with the '%cd' command failed, so the current working directory
# is not set to where the dataset files are located. This code explicitly changes
# the current working directory for this cell's execution.
# Please ensure that '/content/gdrive/MyDrive/classes/COS429/Final Project'
# is the correct absolute path to your dataset within your Google Drive,
# and that your Google Drive is mounted (cell P7w0QyjOuZUK output shows it's mounted).
try:
    dataset_root_path = '/content/gdrive/MyDrive/classes/COS429/Final Project'
    os.chdir(dataset_root_path)
    print(f"Current working directory changed to: {os.getcwd()}")
except FileNotFoundError:
    print("Error: The specified dataset directory was not found.")
    print(f"Please check the path '{dataset_root_path}'")
    print("and ensure your Google Drive is mounted correctly.")
    # Re-raise the error to stop execution, as the dataset won't load correctly otherwise.
    raise

image_transform = transforms.Compose([
    transforms.Resize((256, 512), interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225]) #imagenet mean and std since i am too lazy to calculate them myself
])

mapping = {
        0: 255,
        1: 255,
        2: 255,
        3: 255,
        4: 255,
        5: 255,
        6: 255,
        7: 0,
        8: 1,
        9: 255,
        10: 255,
        11: 2,
        12: 3,
        13: 4,
        14: 255,
        15: 255,
        16: 255,
        17: 5,
        18: 255,
        19: 6,
        20: 7,
        21: 8,
        22: 9,
        23: 10,
        24: 11,
        25: 12,
        26: 13,
        27: 14,
        28: 15,
        29: 255,
        30: 255,
        31: 16,
        32: 17,
        33: 18,
        -1: 255
    }

def convert(c):
  return mapping[c]

def remap_mask(mask):
  return torch.as_tensor(np.vectorize(convert)(mask), dtype=torch.long)

def mask_transform(mask):
  mask = transforms.functional.resize(mask, (256, 512), interpolation=transforms.functional.InterpolationMode.NEAREST)
  mask = remap_mask(mask)
  return mask

'''
mask_transform = transforms.Compose([
    transforms.Resize((256,512), interpolation=transforms.InterpolationMode.NEAREST),
    transforms.PILToTensor(),
])
'''

train_dataset = CityscapesDataset(tvt=0, image_transform=image_transform, mask_transform=mask_transform)
val_dataset = CityscapesDataset(tvt=1,  image_transform=image_transform, mask_transform=mask_transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)

_, mask = train_dataset.__getitem__(0)
mask = mask.cpu().detach().numpy()
print(mask.shape)
print(mask)
print(mask.max())

"""*Training* a U-net"""

import torch

USE_GPU = True
dtype = torch.float32 # We will be using float throughout.

if USE_GPU and torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

print(device)

!pip install segmentation_models_pytorch

import segmentation_models_pytorch as smp

model = smp.Unet(
    encoder_name="resnet18",
    encoder_weights="imagenet",
    classes=19,
    activation=None,
)

for param in model.encoder.parameters():
    param.requires_grad = False

def check_accuracy(loader, model):
    """
    Finds the accuracy of a model

    Inputs:
    - loader: A dataloader containing the validation / testing dataset
    - model: A PyTorch Module giving the model to evaluate.

    Returns: Nothing, but prints the accuracy.
    """

    num_correct = 0
    num_samples = 0
    model.eval()  # set model to evaluation mode

    with torch.no_grad(): # no need to store computation graph or local gradients
        for x, y in loader:
            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU
            y = y.to(device=device, dtype=dtype)
            scores = model(x)
            _, preds = scores.max(1)
            for i in range(preds.size(0)):
              num_correct += (preds[i]==y[i]).sum().item()
            num_samples += preds.size(0)*preds.size(1)*preds.size(2)

        acc = float(num_correct) / num_samples
        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))

class_freq = torch.tensor([0 for i in range(19)])
j = 0
for _,y in train_loader:
  if(j == 10):
    break
  for i in range(19):
    class_freq[i] += (y==i).sum()
  j += 1

class_freq = class_freq.float()

class_weights = torch.tensor([(torch.mean(class_freq)/(class_freq[i] + 1e-6)).item() for i in range(19)])
class_weights = torch.clamp(class_weights, max=1)
print(class_weights)
class_weights /= torch.mean(class_weights)
class_weights = class_weights.to(device)
print(class_weights)

#loss_fn = torch.nn.CrossEntropyLoss()

def train(model, optimizer, loader_train, loader_val, epochs, print_every=10):
    """
    Train a model using the PyTorch Module API.

    Inputs:
    - model: A PyTorch Module giving the model to train.
    - optimizer: An Optimizer object we will use to train the model
    - loader_train: A dataloader containing the train dataset
    - loader_val: A dataloader containing the validation dataset
    - epochs: (Optional) An integer giving the number of epochs to train for
    - print_every: (Optional) An integer specifying how often to print the loss.

    Returns: Nothing, but prints model losses and accuracies during training.
    """
    model = model.to(device=device)  # move the model parameters to CPU/GPU
    losses = []
    for e in range(epochs):
        for t, (x, y) in enumerate(loader_train):
            model.train()  # put model to training mode
            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU
            y = y.to(device=device, dtype=torch.long)

            scores = model(x)
            loss = torch.nn.functional.cross_entropy(scores, y.squeeze(1), ignore_index=255)
            losses.append(loss.item())

            # Zero out all of the gradients for the variables which the optimizer
            # will update.
            optimizer.zero_grad()

            # This is the backwards pass: compute the gradient of the loss with
            # respect to each  parameter of the model.
            loss.backward()

            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) -->

            # Actually update the parameters of the model using the gradients
            # computed by the backwards pass.
            optimizer.step()

            if t % print_every == 0:
                print('Epoch {}, iteration {}, loss = {}'.format(e, t, loss.item()))


        print('Epoch {} done'.format(e))
        check_accuracy(loader_val, model)

    return losses

learning_rate = 1e-5
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

losses = np.array(train(model, optimizer, train_loader, val_loader, epochs=10))

path = "unet_weights5.pth"
torch.save(model.state_dict(), path)

np.save('losses2.npy', losses)

path = "unet_weights.pth"
torch.save(model.state_dict(), path)

import torch
import segmentation_models_pytorch as smp

model = smp.Unet(
    encoder_name="resnet50",
    encoder_weights="imagenet",
    classes=19,
    activation=None,
)

model.load_state_dict(torch.load("unet_weights.pth"))

from PIL import Image
from torchvision import transforms
image = Image.open("leftImg8bit/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png").convert("RGB")
import matplotlib.pyplot as plt
plt.imshow(image)

transform = transforms.Compose([
    transforms.Resize((256,512)),
    transforms.ToTensor()
])

image = transform(image)
image = image.to(device)
model = model.to(device=device)
mask = model(image.unsqueeze(0))
mask = mask.cpu().detach().numpy().squeeze(0)
print(mask[:,0,0])
plt.imshow(mask.argmax(0), cmap="tab20")

"""SegFormer"""

class SegFormerCityscapesDataset(Dataset):
    def __init__(self, tvt, processor):
        self.processor = processor
        self.images = []
        self.masks = []
        self.tvt = tvt  # 0 for train, 1 for val, 2 for test
        if tvt == 0:
            s = "train"
        elif tvt == 1:
            s = "val"
        else:
            s = "test"

        img_root = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
            "data",
            "leftImg8bit",
            s,
        )
        mask_root = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
            "data",
            "gtFine",
            s,
        )

        for city in os.listdir(img_root):
            img_dir = os.path.join(img_root, city)
            mask_dir = os.path.join(mask_root, city)

            for file_name in os.listdir(img_dir):
                if file_name.endswith("_leftImg8bit.png"):
                    img_path = os.path.join(img_dir, file_name)
                    mask_name = file_name.replace(
                        "_leftImg8bit.png", "_gtFine_labelIds.png"
                    )
                    mask_path = os.path.join(mask_dir, mask_name)

                    if os.path.isfile(img_path) and os.path.isfile(mask_path):
                        self.images.append(img_path)
                        self.masks.append(mask_path)

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = Image.open(self.images[idx]).convert("RGB")
        mask = Image.open(self.masks[idx])


        processed = self.processor(images=image, return_tensors="pt")
        pixel_values = processed["pixel_values"].squeeze(0)

        mask = torch.from_numpy(np.array(mask)).long()

        target_size = self.processor.size["height"], self.processor.size["width"]
        mask = transforms.functional.resize(mask.unsqueeze(0), target_size, interpolation=F.InterpolationMode.NEAREST)
        mask = mask.squeeze(0)

        mask = self.id_map[mask]

        return pixel_values, mask

class CityscapesSegFormerDataset(Dataset):
    def __init__(self, tvt, image_transform=None, mask_transform=None):
        self.image_transform = image_transform
        self.mask_transform = mask_transform
        self.images = []
        self.masks = []

        if tvt == 0:
            split = "train"
        elif tvt == 1:
            split = "val"
        else:
            split = "test"

        img_root = os.path.join("leftImg8bit", split)
        mask_root = os.path.join("gtFinetrainvaltest/gtFine", split)

        for city in os.listdir(img_root):
            img_dir = os.path.join(img_root, city)
            mask_dir = os.path.join(mask_root, city)

            for fname in os.listdir(img_dir):
                if fname.endswith("_leftImg8bit.png"):
                    img_path = os.path.join(img_dir, fname)
                    mask_name = fname.replace("_leftImg8bit.png", "_gtFine_labelIds.png")
                    mask_path = os.path.join(mask_dir, mask_name)

                    self.images.append(img_path)
                    self.masks.append(mask_path)

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = Image.open(self.images[idx]).convert("RGB")
        mask = Image.open(self.masks[idx])

        if self.image_transform:
            image = self.image_transform(image)

        if self.mask_transform:
            mask = self.mask_transform(mask)  # Tensor (1,H,W)

        # Convert 0-33 â†’ 0-18 + 255 ignore
        mask_np = mask.squeeze().numpy()
        remapped = np.vectorize(remap_mask)(mask_np)
        mask = torch.tensor(remapped, dtype=torch.long)

        return image, mask

from torch.utils.data import DataLoader
from torchvision import transforms

image_transform = transforms.Compose([
    transforms.Resize((256, 512), interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225]) #imagenet mean and std since i am too lazy to calculate them myself
])

mapping = {
        0: 255,
        1: 255,
        2: 255,
        3: 255,
        4: 255,
        5: 255,
        6: 255,
        7: 0,
        8: 1,
        9: 255,
        10: 255,
        11: 2,
        12: 3,
        13: 4,
        14: 255,
        15: 255,
        16: 255,
        17: 5,
        18: 255,
        19: 6,
        20: 7,
        21: 8,
        22: 9,
        23: 10,
        24: 11,
        25: 12,
        26: 13,
        27: 14,
        28: 15,
        29: 255,
        30: 255,
        31: 16,
        32: 17,
        33: 18,
        -1: 255
    }

def convert(c):
  return mapping[c]

def remap_mask(mask):
  return torch.as_tensor(np.vectorize(convert)(mask), dtype=torch.long)

def mask_transform(mask):
  mask = transforms.functional.resize(mask, (256, 512), interpolation=transforms.functional.InterpolationMode.NEAREST)
  mask = remap_mask(mask)
  return mask

'''
mask_transform = transforms.Compose([
    transforms.Resize((256,512), interpolation=transforms.InterpolationMode.NEAREST),
    transforms.PILToTensor(),
])
'''

train_dataset = CityscapesSegFormerDataset(tvt=0, image_transform=image_transform, mask_transform=mask_transform)
val_dataset = CityscapesSegFormerDataset(tvt=1,  image_transform=image_transform, mask_transform=mask_transform)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)

from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation

model = SegformerForSemanticSegmentation.from_pretrained(
    "nvidia/segformer-b2-finetuned-ade-512-512",
    num_labels=19,
    ignore_mismatched_sizes=True
).to(device)

learning_rate = 1e-5
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

losses = np.array(train(model, optimizer, train_loader, val_loader, epochs=10))

path = "segformer_weights.pth"
torch.save(model.state_dict(), path)

np.save('segformer_losses.npy', losses)

